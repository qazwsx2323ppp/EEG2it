# Qwen2.5-Omni
<p align="left">
        ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="README.md">English</a>
</p>
<br>

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/Omni_logo.png" width="400"/>
<p>

<p align="center">
        ğŸ’œ <a href="https://chat.qwenlm.ai/"><b>Qwen Chat</b></a>&nbsp&nbsp | &nbsp&nbspğŸ¤— <a href="https://huggingface.co/collections/Qwen/qwen25-omni-67de1e5f0f9464dc6314b36e">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href="https://modelscope.cn/collections/Qwen25-Omni-a2505ce0d5514e">ModelScope</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href="https://qwenlm.github.io/blog/qwen2.5-omni/">Blog</a>&nbsp&nbsp | &nbsp&nbspğŸ“š <a href="https://github.com/QwenLM/Qwen2.5-Omni/tree/main/cookbooks">Cookbooks</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href="https://arxiv.org/abs/2503.20215">Paper</a>&nbsp&nbsp
<br>
ğŸ–¥ï¸ <a href="https://modelscope.cn/studios/Qwen/Qwen2.5-Omni-Demo">Demo</a>&nbsp&nbsp | &nbsp&nbspğŸ’¬ <a href="https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbspğŸ«¨ <a href="https://discord.gg/CV4E9rpNSD">Discord</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href="https://help.aliyun.com/zh/model-studio/user-guide/qwen-omni">API</a>
<!-- &nbsp&nbsp | &nbsp&nbspğŸ–¥ï¸ <a href="https://gallery.pai-ml.com/#/preview/deepLearning/cv/qwen2.5-vl">PAI-DSW</a> -->
</p>

**Qwen2.5-Omni**æ­£å¼å‘å¸ƒäº†ï¼è¿™æ˜¯Qwenç³»åˆ—ä¸­å…¨æ–°çš„æ——èˆ°çº§ç«¯åˆ°ç«¯å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œä¸“ä¸ºå…¨é¢çš„å¤šæ¨¡å¼æ„ŸçŸ¥è®¾è®¡ï¼Œæ— ç¼å¤„ç†åŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘åœ¨å†…çš„å„ç§è¾“å…¥ï¼ŒåŒæ—¶æ”¯æŒæµå¼çš„æ–‡æœ¬ç”Ÿæˆå’Œè‡ªç„¶è¯­éŸ³åˆæˆè¾“å‡ºï¼Œè®©æˆ‘ä»¬ç‚¹å‡»ä¸‹æ–¹è§†é¢‘äº†è§£æ›´å¤šä¿¡æ¯å§ ğŸ˜ƒ

<a href="https://youtu.be/UF55yM67EH0" target="_blank">
  <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/video_cover.png" alt="Open Video"/>
</a>

## æ–°é—»
* 2025.06.12: Qwen2.5-Omni-7Båœ¨å£è¯­ç†è§£ä¸æ¨ç†æµ‹è¯„æ¦œå•[MMSU](https://arxiv.org/abs/2506.04779)ä¸­è·å¾—å¼€æºæ¨¡å‹ç¬¬ä¸€ï¼
* 2025.06.09: æ­å–œQwen2.5-Omni-7Båœ¨éŸ³é¢‘ç†è§£ä¸æ¨ç†è¯„æµ‹æ¦œå•[MMAU](https://sakshi113.github.io/mmau_homepage/#leaderboard)ä¸­ä½åˆ—ç¬¬ä¸€ï¼Œåœ¨[MMAR](https://github.com/ddlBoJack/MMAR)è¯„æµ‹ä¸­ä½åˆ—å¼€æºæ¨¡å‹ç¬¬ä¸€ï¼
* 2025.05.16: æˆ‘ä»¬å‘å¸ƒäº†4-bité‡åŒ–çš„Qwen2.5-Omni-7Bï¼ˆGPTQ-Int4/AWQï¼‰ï¼Œå®ƒä¸åŸå§‹ç‰ˆæœ¬ä¿æŒäº†ç›¸ä¼¼çš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘äº†é«˜è¾¾50%+çš„GPUæ˜¾å­˜æ¶ˆè€—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[GPTQ-Int4å’ŒAWQä½¿ç”¨](#gptq-int4-å’Œ-awq-ä½¿ç”¨æ–¹æ³•)ï¼Œæ¨¡å‹å¯ä»¥ä» Hugging Face ([GPTQ-Int4](https://huggingface.co/Qwen/Qwen2.5-Omni-7B-GPTQ-Int4)|[AWQ](https://huggingface.co/Qwen/Qwen2.5-Omni-7B-AWQ)) å’Œ ModelScope ([GPTQ-Int4](https://modelscope.cn/models/Qwen/Qwen2.5-Omni-7B-GPTQ-Int4)|[AWQ](https://modelscope.cn/models/Qwen/Qwen2.5-Omni-7B-AWQ)) ä¸­è·å–ã€‚
* 2025.05.13: [MNN Chat App](https://github.com/alibaba/MNN/blob/master/apps/Android/MnnLlmChat/README.md#releases) ç›®å‰å·²ç»æ”¯æŒQwen2.5-Omniäº†, è®©æˆ‘ä»¬åœ¨ç«¯ä¾§è®¾å¤‡ä¸­ä½“éªŒQwen2.5-Omniå§ï¼è¯·è®¿é—®[ä½¿ç”¨MNNéƒ¨ç½²](#ä½¿ç”¨mnnéƒ¨ç½² )æ¥è·å–æœ‰å…³æ¨¡å‹å†…å­˜æ¶ˆè€—å’Œæ¨ç†é€Ÿåº¦çš„åŸºå‡†æµ‹è¯•çš„ä¿¡æ¯ã€‚
* 2025.04.30: ä»¤äººæ¿€åŠ¨ï¼æˆ‘ä»¬å‘å¸ƒäº†Qwen2.5-Omni-3Bï¼Œä»¥ä¾¿äºæ›´å¤šçš„å¹³å°èƒ½å¤Ÿè¿è¡ŒQwen2.5-Omniï¼Œæ¨¡å‹ç›®å‰å¯ä»¥åœ¨[Hugging Face](https://huggingface.co/Qwen/Qwen2.5-Omni-3B)ä¸­ä¸‹è½½ï¼Œè¯¥æ¨¡å‹çš„[æ€§èƒ½æŒ‡æ ‡](#æ€§èƒ½æŒ‡æ ‡)ä¿¡æ¯å·²ç»æ›´æ–°ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡[æœ€å°æ˜¾å­˜å ç”¨ä¿¡æ¯](#æœ€å°gpuå†…å­˜éœ€æ±‚)æ¥äº†è§£èµ„æºéœ€æ±‚ã€‚ä¸ºäº†æ›´å¥½çš„ä½“éªŒï¼Œ[transformers](#--transformers-usage)å’Œ[vllm](#deployment-with-vllm)ä»£ç å·²ç»æ›´æ–°ï¼Œæ‚¨å¯ä»¥é‡æ–°æ‹‰å–æœ€æ–°çš„[å®˜æ–¹é•œåƒ](#-docker)æ¥è·å–ä»–ä»¬ã€‚
* 2025.04.11: æˆ‘ä»¬æ­£å¼å‘å¸ƒäº†æ”¯æŒéŸ³é¢‘è¾“å‡ºçš„vllmç‰ˆæœ¬ï¼è¯·ä»æºç æˆ–è€…æˆ‘ä»¬çš„é•œåƒä¸­æ¥ä½“éªŒã€‚
* 2025.04.02: â­ï¸â­ï¸â­ï¸ Qwen2.5-Omni è¾¾åˆ° Hugging Face Trending æ¦œçš„ top-1! 
* 2025.03.29: â­ï¸â­ï¸â­ï¸ Qwen2.5-Omni è¾¾åˆ° Hugging Face Trending æ¦œçš„ top-2! 
* 2025.03.26: ä¸Qwen2.5-Omniçš„å®æ—¶äº¤äº’ä½“éªŒå·²ç»åœ¨ [Qwen Chat](https://chat.qwen.ai/) ä¸Šçº¿ï¼Œæ¬¢è¿ä½“éªŒ!
* 2025.03.26: æˆ‘ä»¬å‘å¸ƒäº† [Qwen2.5-Omni](https://huggingface.co/collections/Qwen/qwen25-omni-67de1e5f0f9464dc6314b36e). å¯¹äºæ›´å¤šçš„ä¿¡æ¯è¯·è®¿é—®æˆ‘ä»¬çš„[åšå®¢](https://qwenlm.github.io/zh/blog/qwen2.5-omni/)!


## ç›®å½• <!-- omit in toc -->

- [æ¦‚è§ˆ](#æ¦‚è§ˆ)
  - [ç®€ä»‹](#ç®€ä»‹)
  - [ä¸»è¦ç‰¹ç‚¹](#ä¸»è¦ç‰¹ç‚¹)
  - [æ¨¡å‹ç»“æ„](#æ¨¡å‹ç»“æ„)
  - [æ€§èƒ½æŒ‡æ ‡](#æ€§èƒ½æŒ‡æ ‡)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
  - [Transformers ä½¿ç”¨æ–¹æ³•](#--transformers-ä½¿ç”¨æ–¹æ³•)
  - [ModelScope ä½¿ç”¨æ–¹æ³•](#-modelscope-ä½¿ç”¨æ–¹æ³•)
  - [GPTQ-Int4 å’Œ AWQ ä½¿ç”¨æ–¹æ³•](#gptq-int4-å’Œ-awq-ä½¿ç”¨æ–¹æ³•)
  - [ä½¿ç”¨æç¤º](#ä½¿ç”¨æç¤º)
  - [æ›´å¤šä½¿ç”¨ç”¨ä¾‹çš„ Cookbooks](#æ›´å¤šä½¿ç”¨ç”¨ä¾‹çš„-cookbooks)
  - [API æ¨ç†](#api-æ¨ç†)
  - [è‡ªå®šä¹‰æ¨¡å‹è®¾å®š](#è‡ªå®šä¹‰æ¨¡å‹è®¾å®š)
- [å’Œ Qwen2.5-Omni å¯¹è¯](#å’Œ-qwen25-omni-å¯¹è¯)
  - [åœ¨çº¿æ¼”ç¤º](#åœ¨çº¿æ¼”ç¤º)
  - [å¯åŠ¨æœ¬åœ°ç½‘é¡µæ¼”ç¤º](#å¯åŠ¨æœ¬åœ°ç½‘é¡µæ¼”ç¤º)
  - [å®æ—¶äº¤äº’](#å®æ—¶äº¤äº’)
- [ä½¿ç”¨vLLMéƒ¨ç½²](#ä½¿ç”¨vLLMéƒ¨ç½²)
- [ä½¿ç”¨MNNéƒ¨ç½²](#ä½¿ç”¨mnnéƒ¨ç½²)
- [Docker](#-docker)
<!-- - [å¼•ç”¨](#citation) -->

## æ¦‚è§ˆ
### ç®€ä»‹
Qwen 2.5-Omniæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æ„ŸçŸ¥åŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘åœ¨å†…çš„å¤šç§æ¨¡æ€ï¼ŒåŒæ—¶ä»¥æµå¼çš„æ–¹å¼ç”Ÿæˆæ–‡æœ¬å’Œè‡ªç„¶è¯­éŸ³å“åº”ã€‚

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/qwen_omni.png" width="80%"/>
<p>

### ä¸»è¦ç‰¹ç‚¹

* **å…¨èƒ½åˆ›æ–°æ¶æ„**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨æ–°çš„Thinker-Talkeræ¶æ„ï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ—¨åœ¨æ”¯æŒæ–‡æœ¬/å›¾åƒ/éŸ³é¢‘/è§†é¢‘çš„è·¨æ¨¡æ€ç†è§£ï¼ŒåŒæ—¶ä»¥æµå¼æ–¹å¼ç”Ÿæˆæ–‡æœ¬å’Œè‡ªç„¶è¯­éŸ³å“åº”ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä½ç½®ç¼–ç æŠ€æœ¯ï¼Œç§°ä¸ºTMRoPEï¼ˆTime-aligned Multimodal RoPEï¼‰ï¼Œé€šè¿‡æ—¶é—´è½´å¯¹é½å®ç°è§†é¢‘ä¸éŸ³é¢‘è¾“å…¥çš„ç²¾å‡†åŒæ­¥ã€‚

* **å®æ—¶éŸ³è§†é¢‘äº¤äº’**ï¼šæ¶æ„æ—¨åœ¨æ”¯æŒå®Œå…¨å®æ—¶äº¤äº’ï¼Œæ”¯æŒåˆ†å—è¾“å…¥å’Œå³æ—¶è¾“å‡ºã€‚

* **è‡ªç„¶æµç•…çš„è¯­éŸ³ç”Ÿæˆ**ï¼šåœ¨è¯­éŸ³ç”Ÿæˆçš„è‡ªç„¶æ€§å’Œç¨³å®šæ€§æ–¹é¢è¶…è¶Šäº†è®¸å¤šç°æœ‰çš„æµå¼å’Œéæµå¼æ›¿ä»£æ–¹æ¡ˆã€‚

* **å…¨æ¨¡æ€æ€§èƒ½ä¼˜åŠ¿**ï¼šåœ¨åŒç­‰è§„æ¨¡çš„å•æ¨¡æ€æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•æ—¶ï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚Qwen2.5-Omniåœ¨éŸ³é¢‘èƒ½åŠ›ä¸Šä¼˜äºç±»ä¼¼å¤§å°çš„Qwen2-Audioï¼Œå¹¶ä¸Qwen2.5-VL-7Bä¿æŒåŒç­‰æ°´å¹³ã€‚

* **å“è¶Šçš„ç«¯åˆ°ç«¯è¯­éŸ³æŒ‡ä»¤è·Ÿéšèƒ½åŠ›**ï¼šQwen2.5-Omniåœ¨ç«¯åˆ°ç«¯è¯­éŸ³æŒ‡ä»¤è·Ÿéšæ–¹é¢è¡¨ç°å‡ºä¸æ–‡æœ¬è¾“å…¥å¤„ç†ç›¸åª²ç¾çš„æ•ˆæœï¼Œåœ¨MMLUé€šç”¨çŸ¥è¯†ç†è§£å’ŒGSM8Kæ•°å­¦æ¨ç†ç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

### æ¨¡å‹ç»“æ„

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/overview.png" width="80%"/>
<p>

### æ€§èƒ½æŒ‡æ ‡

Qwen2.5-Omniåœ¨åŒ…æ‹¬å›¾åƒï¼ŒéŸ³é¢‘ï¼ŒéŸ³è§†é¢‘ç­‰å„ç§æ¨¡æ€ä¸‹çš„è¡¨ç°éƒ½ä¼˜äºç±»ä¼¼å¤§å°çš„å•æ¨¡æ€æ¨¡å‹ä»¥åŠå°é—­æºæ¨¡å‹ï¼Œä¾‹å¦‚Qwen2.5-VL-7Bã€Qwen2-Audioå’ŒGemini-1.5-proã€‚åœ¨å¤šæ¨¡æ€ä»»åŠ¡OmniBenchï¼ŒQwen2.5-Omniè¾¾åˆ°äº†SOTAçš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œåœ¨å•æ¨¡æ€ä»»åŠ¡ä¸­ï¼ŒQwen2.5-Omniåœ¨å¤šä¸ªé¢†åŸŸä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ï¼ˆCommon Voiceï¼‰ã€ç¿»è¯‘ï¼ˆCoVoST2ï¼‰ã€éŸ³é¢‘ç†è§£ï¼ˆMMAUï¼‰ã€å›¾åƒæ¨ç†ï¼ˆMMMUã€MMStarï¼‰ã€è§†é¢‘ç†è§£ï¼ˆMVBenchï¼‰ä»¥åŠè¯­éŸ³ç”Ÿæˆï¼ˆSeed-tts-evalå’Œä¸»è§‚è‡ªç„¶å¬æ„Ÿï¼‰ã€‚

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/bar.png"/>
<p>

<details>
<summary>Multimodality  -> Text</summary>

<table class="tg"><thead>
  <tr>
    <th class="tg-0lax">Datasets</th>
    <th class="tg-0lax">Model</th>
    <th class="tg-0lax">Performance</th>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-0lax" rowspan="10">OmniBench<br>Speech | Sound Event | Music | Avg</td>
    <td class="tg-0lax">Gemini-1.5-Pro</td>
    <td class="tg-0lax">42.67%|42.26%|46.23%|42.91%</td>
  </tr>
  <tr>
    <td class="tg-0lax">MIO-Instruct</td>
    <td class="tg-0lax">36.96%|33.58%|11.32%|33.80%</td>
  </tr>
  <tr>
    <td class="tg-0lax">AnyGPT (7B)</td>
    <td class="tg-0lax">17.77%|20.75%|13.21%|18.04%</td>
  </tr>
  <tr>
    <td class="tg-0lax">video-SALMONN</td>
    <td class="tg-0lax">34.11%|31.70%|<strong>56.60%</strong>|35.64%</td>
  </tr>
  <tr>
    <td class="tg-0lax">UnifiedIO2-xlarge</td>
    <td class="tg-0lax">39.56%|36.98%|29.25%|38.00%</td>
  </tr>
  <tr>
    <td class="tg-0lax">UnifiedIO2-xxlarge</td>
    <td class="tg-0lax">34.24%|36.98%|24.53%|33.98%</td>
  </tr>
  <tr>
    <td class="tg-0lax">MiniCPM-o</td>
    <td class="tg-0lax">-|-|-|40.50%</td>
  </tr>
  <tr>
    <td class="tg-0lax">Baichuan-Omni-1.5</td>
    <td class="tg-0lax">-|-|-|42.90%</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-3B</td>
    <td class="tg-0lax">52.14%|52.08%|52.83%|52.19%</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax"><strong>55.25%</strong>|<strong>60.00%</strong>|52.83%|<strong>56.13%</strong></td>
  </tr>
</tbody></table>
</details>


<details>
<summary>Audio -> Text</summary>


<table class="tg"><thead>
  <tr>
    <th class="tg-0lax">Datasets</th>
    <th class="tg-0lax">Model</th>
    <th class="tg-0lax">Performance</th>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-9j4x" colspan="3">ASR</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="12">Librispeech<br>dev-clean | dev other | test-clean | test-other</td>
    <td class="tg-0lax">SALMONN</td>
    <td class="tg-0lax">-|-|2.1|4.9</td>
  </tr>
  <tr>
    <td class="tg-0lax">SpeechVerse</td>
    <td class="tg-0lax">-|-|2.1|4.4</td>
  </tr>
  <tr>
    <td class="tg-0lax">Whisper-large-v3</td>
    <td class="tg-0lax">-|-|1.8|3.6</td>
  </tr>
  <tr>
    <td class="tg-0lax">Llama-3-8B</td>
    <td class="tg-0lax">-|-|-|3.4</td>
  </tr>
  <tr>
    <td class="tg-0lax">Llama-3-70B</td>
    <td class="tg-0lax">-|-|-|3.1</td>
  </tr>
  <tr>
    <td class="tg-0lax">Seed-ASR-Multilingual</td>
    <td class="tg-0lax">-|-|<strong>1.6</strong>|<strong>2.8</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax">MiniCPM-o</td>
    <td class="tg-0lax">-|-|1.7|-</td>
  </tr>
  <tr>
    <td class="tg-0lax">MinMo</td>
    <td class="tg-0lax">-|-|1.7|3.9</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen-Audio</td>
    <td class="tg-0lax">1.8|4.0|2.0|4.2</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2-Audio</td>
    <td class="tg-0lax"><strong>1.3</strong>|<strong>3.4</strong>|<strong>1.6</strong>|3.6</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-3B</td>
    <td class="tg-0lax">2.0|4.1|2.2|4.5</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax">1.6|3.5|1.8|3.4</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="5">Common Voice 15<br>en | zh | yue | fr</td>
    <td class="tg-0lax">Whisper-large-v3</td>
    <td class="tg-0lax">9.3|12.8|10.9|10.8</td>
  </tr>
  <tr>
    <td class="tg-0lax">MinMo</td>
    <td class="tg-0lax">7.9|6.3|6.4|8.5</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2-Audio</td>
    <td class="tg-0lax">8.6|6.9|<strong>5.9</strong>|9.6</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-3B</td>
    <td class="tg-0lax">9.1|6.0|11.6|9.6</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax"><strong>7.6</strong>|<strong>5.2</strong>|7.3|<strong>7.5</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="8">Fleurs<br>zh | en</td>
    <td class="tg-0lax">Whisper-large-v3</td>
    <td class="tg-0lax">7.7|4.1</td>
  </tr>
  <tr>
    <td class="tg-0lax">Seed-ASR-Multilingual</td>
    <td class="tg-0lax">-|<strong>3.4</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax">Megrez-3B-Omni</td>
    <td class="tg-0lax">10.8|-</td>
  </tr>
  <tr>
    <td class="tg-0lax">MiniCPM-o</td>
    <td class="tg-0lax">4.4|-</td>
  </tr>
  <tr>
    <td class="tg-0lax">MinMo</td>
    <td class="tg-0lax">3.0|3.8</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2-Audio</td>
    <td class="tg-0lax">7.5|-</td>
  </tr>
    <tr>
    <td class="tg-0lax">Qwen2.5-Omni-3B</td>
    <td class="tg-0lax">3.2|5.4</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax"><strong>3.0</strong>|4.1</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="6">Wenetspeech<br>test-net | test-meeting</td>
    <td class="tg-0lax">Seed-ASR-Chinese</td>
    <td class="tg-0lax"><strong>4.7|5.7</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax">Megrez-3B-Omni</td>
    <td class="tg-0lax">-|16.4</td>
  </tr>
  <tr>
    <td class="tg-0lax">MiniCPM-o</td>
    <td class="tg-0lax">6.9|-</td>
  </tr>
  <tr>
    <td class="tg-0lax">MinMo</td>
    <td class="tg-0lax">6.8|7.4</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-3B</td>
    <td class="tg-0lax">6.3|8.1</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax">5.9|7.7</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="4">Voxpopuli-V1.0-en</td>
    <td class="tg-0lax">Llama-3-8B</td>
    <td class="tg-0lax">6.2</td>
  </tr>
  <tr>
    <td class="tg-0lax">Llama-3-70B</td>
    <td class="tg-0lax"><strong>5.7</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-3B</td>
    <td class="tg-0lax">6.6</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax">5.8</td>
  </tr>
  <tr>
    <td class="tg-9j4x" colspan="3">S2TT</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="9">CoVoST2<br>en-de | de-en | en-zh | zh-en</td>
    <td class="tg-0lax">SALMONN</td>
    <td class="tg-0lax">18.6|-|33.1|-</td>
  </tr>
  <tr>
    <td class="tg-0lax">SpeechLLaMA</td>
    <td class="tg-0lax">-|27.1|-|12.3</td>
  </tr>
  <tr>
    <td class="tg-0lax">BLSP</td>
    <td class="tg-0lax">14.1|-|-|-</td>
  </tr>
  <tr>
    <td class="tg-0lax">MiniCPM-o</td>
    <td class="tg-0lax">-|-|<strong>48.2</strong>|27.2</td>
  </tr>
  <tr>
    <td class="tg-0lax">MinMo</td>
    <td class="tg-0lax">-|<strong>39.9</strong>|46.7|26.0</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen-Audio</td>
    <td class="tg-0lax">25.1|33.9|41.5|15.7</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2-Audio</td>
    <td class="tg-0lax">29.9|35.2|45.2|24.4</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-3B</td>
    <td class="tg-0lax">28.3|38.1|41.4|26.6</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax"><strong>30.2</strong>|37.7|41.4|<strong>29.4</strong></td>
  </tr>
  <tr>
    <td class="tg-9j4x" colspan="3">SER</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="6">Meld</td>
    <td class="tg-0lax">WavLM-large</td>
    <td class="tg-0lax">0.542</td>
  </tr>
  <tr>
    <td class="tg-0lax">MiniCPM-o</td>
    <td class="tg-0lax">0.524</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen-Audio</td>
    <td class="tg-0lax">0.557</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2-Audio</td>
    <td class="tg-0lax">0.553</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-3B</td>
    <td class="tg-0lax">0.558</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax"><strong>0.570</strong></td>
  </tr>
  <tr>
    <td class="tg-9j4x" colspan="3">VSC</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="6">VocalSound</td>
    <td class="tg-0lax">CLAP</td>
    <td class="tg-0lax">0.495</td>
  </tr>
  <tr>
    <td class="tg-0lax">Pengi</td>
    <td class="tg-0lax">0.604</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen-Audio</td>
    <td class="tg-0lax">0.929</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2-Audio</td>
    <td class="tg-0lax"><strong>0.939</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-3B</td>
    <td class="tg-0lax">0.936</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax"><strong>0.939</strong></td>
  </tr>
  <tr>
    <td class="tg-9j4x" colspan="3">Music</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="3">GiantSteps Tempo</td>
    <td class="tg-0lax">Llark-7B</td>
    <td class="tg-0lax">0.86</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-3B</td>
    <td class="tg-0lax"><strong>0.88</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax"><strong>0.88</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="3">MusicCaps</td>
    <td class="tg-0lax">LP-MusicCaps</td>
    <td class="tg-0lax">0.291|0.149|0.089|<strong>0.061</strong>|0.129|0.130</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-3B</td>
    <td class="tg-0lax">0.325|<strong>0.163</strong>|<strong>0.093</strong>|0.057|<strong>0.132</strong>|<strong>0.229</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax"><strong>0.328</strong>|0.162|0.090|0.055|0.127|0.225</td>
  </tr>
  <tr>
    <td class="tg-9j4x" colspan="3">Audio Reasoning</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="4">MMAU<br>Sound | Music | Speech | Avg</td>
    <td class="tg-0lax">Gemini-Pro-V1.5</td>
    <td class="tg-0lax">56.75|49.40|58.55|54.90</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2-Audio</td>
    <td class="tg-0lax">54.95|50.98|42.04|49.20</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-3B</td>
    <td class="tg-0lax"><strong>70.27</strong>|60.48|59.16|63.30</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax">67.87|<strong>69.16|59.76|65.60</strong></td>
  </tr>
  <tr>
    <td class="tg-9j4x" colspan="3">Voice Chatting</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="9">VoiceBench<br>AlpacaEval | CommonEval | SD-QA | MMSU</td>
    <td class="tg-0lax">Ultravox-v0.4.1-LLaMA-3.1-8B</td>
    <td class="tg-0lax"><strong>4.55</strong>|3.90|53.35|47.17</td>
  </tr>
  <tr>
    <td class="tg-0lax">MERaLiON</td>
    <td class="tg-0lax">4.50|3.77|55.06|34.95</td>
  </tr>
  <tr>
    <td class="tg-0lax">Megrez-3B-Omni</td>
    <td class="tg-0lax">3.50|2.95|25.95|27.03</td>
  </tr>
  <tr>
    <td class="tg-0lax">Lyra-Base</td>
    <td class="tg-0lax">3.85|3.50|38.25|49.74</td>
  </tr>
  <tr>
    <td class="tg-0lax">MiniCPM-o</td>
    <td class="tg-0lax">4.42|<strong>4.15</strong>|50.72|54.78</td>
  </tr>
  <tr>
    <td class="tg-0lax">Baichuan-Omni-1.5</td>
    <td class="tg-0lax">4.50|4.05|43.40|57.25</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2-Audio</td>
    <td class="tg-0lax">3.74|3.43|35.71|35.72</td>
  </tr>
    <tr>
    <td class="tg-0lax">Qwen2.5-Omni-3B</td>
    <td class="tg-0lax">4.32|4.00|49.37|50.23</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax">4.49|3.93|<strong>55.71</strong>|<strong>61.32</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="9">VoiceBench<br>OpenBookQA | IFEval | AdvBench | Avg</td>
    <td class="tg-0lax">Ultravox-v0.4.1-LLaMA-3.1-8B</td>
    <td class="tg-0lax">65.27|<strong>66.88</strong>|98.46|71.45</td>
  </tr>
  <tr>
    <td class="tg-0lax">MERaLiON</td>
    <td class="tg-0lax">27.23|62.93|94.81|62.91</td>
  </tr>
  <tr>
    <td class="tg-0lax">Megrez-3B-Omni</td>
    <td class="tg-0lax">28.35|25.71|87.69|46.25</td>
  </tr>
  <tr>
    <td class="tg-0lax">Lyra-Base</td>
    <td class="tg-0lax">72.75|36.28|59.62|57.66</td>
  </tr>
  <tr>
    <td class="tg-0lax">MiniCPM-o</td>
    <td class="tg-0lax">78.02|49.25|97.69|71.69</td>
  </tr>
  <tr>
    <td class="tg-0lax">Baichuan-Omni-1.5</td>
    <td class="tg-0lax">74.51|54.54|97.31|71.14</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2-Audio</td>
    <td class="tg-0lax">49.45|26.33|96.73|55.35</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-3B</td>
    <td class="tg-0lax">74.73|42.10|98.85|68.81</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax"><strong>81.10</strong>|52.87|<strong>99.42</strong>|<strong>74.12</strong></td>
  </tr>
</tbody></table>
</details>

<details>
<summary>Image -> Text</summary>

| Dataset                        | Qwen2.5-Omni-7B | Qwen2.5-Omni-3B | Other Best | Qwen2.5-VL-7B | GPT-4o-mini | 
|--------------------------------|--------------|------------|------------|---------------|-------------|
| MMMU<sub>val</sub>             | 59.2         | 53.1       | 53.9       | 58.6          | **60.0**    | 
| MMMU-Pro<sub>overall</sub>     | 36.6         | 29.7       | -          | **38.3**      | 37.6        | 
| MathVista<sub>testmini</sub>   | 67.9         | 59.4       | **71.9**   | 68.2          | 52.5        | 
| MathVision<sub>full</sub>      | 25.0         | 20.8       | 23.1       | **25.1**      | -           | 
| MMBench-V1.1-EN<sub>test</sub> | 81.8         | 77.8       | 80.5       | **82.6**      | 76.0        | 
| MMVet<sub>turbo</sub>          | 66.8         | 62.1       | **67.5**   | 67.1          | 66.9        | 
| MMStar                         | **64.0**     | 55.7       | **64.0**   | 63.9          | 54.8        | 
| MME<sub>sum</sub>              | 2340         | 2117       | **2372**   | 2347          | 2003        | 
| MuirBench                      | 59.2         | 48.0       | -          | **59.2**      | -           | 
| CRPE<sub>relation</sub>        | **76.5**     | 73.7       | -          | 76.4          | -           | 
| RealWorldQA<sub>avg</sub>      | 70.3         | 62.6       | **71.9**   | 68.5          | -           | 
| MME-RealWorld<sub>en</sub>     | **61.6**     | 55.6       | -          | 57.4          | -           | 
| MM-MT-Bench                    | 6.0          | 5.0        | -          | **6.3**       | -           | 
| AI2D                           | 83.2         | 79.5       | **85.8**   | 83.9          | -           | 
| TextVQA<sub>val</sub>          | 84.4         | 79.8       | 83.2       | **84.9**      | -           | 
| DocVQA<sub>test</sub>          | 95.2         | 93.3       | 93.5       | **95.7**      | -           | 
| ChartQA<sub>test Avg</sub>     | 85.3         | 82.8       | 84.9       | **87.3**      | -           | 
| OCRBench_V2<sub>en</sub>       | **57.8**     | 51.7       | -          | 56.3          | -           | 


| Dataset                  | Qwen2.5-Omni-7B | Qwen2.5-Omni-3B | Qwen2.5-VL-7B | Grounding DINO | Gemini 1.5 Pro | 
|--------------------------|--------------|---------------|---------------|----------------|----------------|
| Refcoco<sub>val</sub>    | 90.5         | 88.7          | 90.0          | **90.6**       | 73.2           | 
| Refcoco<sub>textA</sub>  | **93.5**     | 91.8          | 92.5          | 93.2           | 72.9           | 
| Refcoco<sub>textB</sub>  | 86.6         | 84.0          | 85.4          | **88.2**       | 74.6           | 
| Refcoco+<sub>val</sub>   | 85.4         | 81.1          | 84.2          | **88.2**       | 62.5           | 
| Refcoco+<sub>textA</sub> | **91.0**     | 87.5          | 89.1          | 89.0           | 63.9           | 
| Refcoco+<sub>textB</sub> | **79.3**     | 73.2          | 76.9          | 75.9           | 65.0           | 
| Refcocog+<sub>val</sub>  | **87.4**     | 85.0          | 87.2          | 86.1           | 75.2           | 
| Refcocog+<sub>test</sub> | **87.9**     | 85.1          | 87.2          | 87.0           | 76.2           | 
| ODinW                    | 42.4         | 39.2          | 37.3          | **55.0**       | 36.7           | 
| PointGrounding           | 66.5         | 46.2          | **67.3**      | -              | -              | 
</details>


<details>
<summary>Video(without audio) -> Text</summary>

| Dataset                     | Qwen2.5-Omni-7B | Qwen2.5-Omni-3B | Other Best | Qwen2.5-VL-7B | GPT-4o-mini | 
|-----------------------------|--------------|------------|------------|---------------|-------------|
| Video-MME<sub>w/o sub</sub> | 64.3         | 62.0       | 63.9       | **65.1**      | 64.8        | 
| Video-MME<sub>w sub</sub>   | **72.4**     | 68.6       | 67.9       | 71.6          | -           | 
| MVBench                     | **70.3**     | 68.7       | 67.2       | 69.6          | -           | 
| EgoSchema<sub>test</sub>    | **68.6**     | 61.4       | 63.2       | 65.0          | -           | 
</details>

<details>
<summary>Zero-shot Speech Generation</summary>


<table class="tg"><thead>
  <tr>
    <th class="tg-0lax">Datasets</th>
    <th class="tg-0lax">Model</th>
    <th class="tg-0lax">Performance</th>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-9j4x" colspan="3">Content Consistency</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="11">SEED<br>test-zh | test-en | test-hard </td>
    <td class="tg-0lax">Seed-TTS_ICL</td>
    <td class="tg-0lax">1.11 | 2.24 | 7.58</td>
  </tr>
  <tr>
    <td class="tg-0lax">Seed-TTS_RL</td>
    <td class="tg-0lax"><strong>1.00</strong> | 1.94 | <strong>6.42</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax">MaskGCT</td>
    <td class="tg-0lax">2.27 | 2.62 | 10.27</td>
  </tr>
  <tr>
    <td class="tg-0lax">E2_TTS</td>
    <td class="tg-0lax">1.97 | 2.19 | -</td>
  </tr>
  <tr>
    <td class="tg-0lax">F5-TTS</td>
    <td class="tg-0lax">1.56 | <strong>1.83</strong> | 8.67</td>
  </tr>
  <tr>
    <td class="tg-0lax">CosyVoice 2</td>
    <td class="tg-0lax">1.45 | 2.57 | 6.83</td>
  </tr>
  <tr>
    <td class="tg-0lax">CosyVoice 2-S</td>
    <td class="tg-0lax">1.45 | 2.38 | 8.08</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-3B_ICL</td>
    <td class="tg-0lax">1.95 | 2.87 | 9.92</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-3B_RL</td>
    <td class="tg-0lax">1.58 | 2.51 | 7.86</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B_ICL</td>
    <td class="tg-0lax">1.70 | 2.72 | 7.97</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B_RL</td>
    <td class="tg-0lax">1.42 | 2.32 | 6.54</td>
  </tr>
  <tr>
    <td class="tg-9j4x" colspan="3">Speaker Similarity</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="11">SEED<br>test-zh | test-en | test-hard </td>
    <td class="tg-0lax">Seed-TTS_ICL</td>
    <td class="tg-0lax">0.796 | 0.762 | 0.776</td>
  </tr>
  <tr>
    <td class="tg-0lax">Seed-TTS_RL</td>
    <td class="tg-0lax"><strong>0.801</strong> | <strong>0.766</strong> | <strong>0.782</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax">MaskGCT</td>
    <td class="tg-0lax">0.774 | 0.714 | 0.748</td>
  </tr>
  <tr>
    <td class="tg-0lax">E2_TTS</td>
    <td class="tg-0lax">0.730 | 0.710 | -</td>
  </tr>
  <tr>
    <td class="tg-0lax">F5-TTS</td>
    <td class="tg-0lax">0.741 | 0.647 | 0.713</td>
  </tr>
  <tr>
    <td class="tg-0lax">CosyVoice 2</td>
    <td class="tg-0lax">0.748 | 0.652 | 0.724</td>
  </tr>
  <tr>
    <td class="tg-0lax">CosyVoice 2-S</td>
    <td class="tg-0lax">0.753 | 0.654 | 0.732</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-3B_ICL</td>
    <td class="tg-0lax">0.741 | 0.635 | 0.748</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-3B_RL</td>
    <td class="tg-0lax">0.744 | 0.635 | 0.746</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B_ICL</td>
    <td class="tg-0lax">0.752 | 0.632 | 0.747</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B_RL</td>
    <td class="tg-0lax">0.754 | 0.641 | 0.752</td>
  </tr>
</tbody></table>
</details>

<details>
<summary>Text -> Text</summary>

| Dataset                           | Qwen2.5-Omni-7B | Qwen2.5-Omni-3B | Qwen2.5-7B | Qwen2.5-3B | Qwen2-7B | Llama3.1-8B | Gemma2-9B | 
|-----------------------------------|-----------|------------|------------|------------|------------|-------------|-----------|
| MMLU-Pro                          | 47.0      | 40.4       | **56.3**   | 43.7       | 44.1       | 48.3        | 52.1      | 
| MMLU-redux                        | 71.0      | 60.9       | **75.4**   | 64.4       | 67.3       | 67.2        | 72.8      | 
| LiveBench<sub>0831</sub>          | 29.6      | 22.3       | **35.9**   | 26.8       | 29.2       | 26.7        | 30.6      | 
| GPQA                              | 30.8      | 34.3       | **36.4**   | 30.3       | 34.3       | 32.8        | 32.8      | 
| MATH                              | 71.5      | 63.6       | **75.5**   | 65.9       | 52.9       | 51.9        | 44.3      | 
| GSM8K                             | 88.7      | 82.6       | **91.6**   | 86.7       | 85.7       | 84.5        | 76.7      | 
| HumanEval                         | 78.7      | 70.7       | **84.8**   |	74.4       | 79.9       | 72.6        | 68.9      | 
| MBPP                              | 73.2      | 70.4       | **79.2**   | 72.7       | 67.2       | 69.6        | 74.9      | 
| MultiPL-E                         | 65.8      | 57.6       | **70.4**   | 60.2       | 59.1       | 50.7        | 53.4      | 
| LiveCodeBench<sub>2305-2409</sub> | 24.6      | 16.5       | **28.7**   | 19.9       | 23.9       | 8.3         | 18.9      | 
</details>

## å¿«é€Ÿå¼€å§‹

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æä¾›å¦‚ä½•åœ¨ğŸ¤– ModelScopeå’ŒğŸ¤— Transformersä¸Šä½¿ç”¨ Qwen2.5-Omni. Qwen2.5-Omniçš„ä»£ç åœ¨Hugging Face transformersçš„æœ€æ–°ç‰ˆæœ¬ä¸­ï¼Œæ‚¨å¯ä»¥ç›´æ¥é€šè¿‡ä¸‹é¢çš„å‘½ä»¤å®‰è£…ï¼š
```
pip install transformers==4.52.3
pip install accelerate
```
å¦åˆ™æ‚¨å¯èƒ½ä¼šé‡åˆ°ä»¥ä¸‹é”™è¯¯ï¼š
```
KeyError: 'qwen2_5_omni'
```
æˆ–è€…æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„[å®˜æ–¹ docker é•œåƒ](#-docker)æ¥å…å»ä»æºç æ„å»ºã€‚

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªé¢å¤–çš„å°å·¥å…·ï¼Œå®ƒä½¿æ‚¨å¯ä»¥æ›´æ–¹ä¾¿åœ°åƒä½¿ç”¨APIä¸€æ ·å¤„ç†å„ç§éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€çš„è¾“å…¥ï¼Œå¤„ç†æ‚¨è¾“å…¥ä¸­åŒ…æ‹¬base64ã€URLä»¥åŠäº¤é”™çš„éŸ³é¢‘ã€å›¾åƒå’Œè§†é¢‘çš„æƒ…å†µã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…æ­¤å·¥å…·åŒ…ï¼Œå¹¶ç¡®ä¿æ‚¨çš„ç³»ç»Ÿå®‰è£…äº†`ffmpeg`ï¼š

```bash
# éå¸¸å»ºè®®ä½¿ç”¨`[decord]`ç‰¹æ€§æ¥è·å–æ›´å¿«çš„è§†é¢‘è¯»å–é€Ÿåº¦
pip install qwen-omni-utils[decord] -U
```

å¦‚æœæ‚¨æœªä½¿ç”¨Linuxæ“ä½œç³»ç»Ÿï¼Œæ‚¨å¯èƒ½æ— æ³•ä»PyPIå®‰è£…`decord`ã€‚ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`pip install qwen-omni-utils -U`ï¼Œå®ƒå°†å›é€€åˆ°ä½¿ç”¨torchvisionè¿›è¡Œè§†é¢‘å¤„ç†ã€‚ ä½†æ˜¯ï¼Œæ‚¨ä»ç„¶å¯ä»¥[ä»æºä»£ç å®‰è£…decord](https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source)ï¼Œä»¥åœ¨åŠ è½½è§†é¢‘æ—¶ä½¿ç”¨decordã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‡†å¤‡äº†ä¸€äº› [cookbooks](https://github.com/QwenLM/Qwen2.5-Omni/tree/main/cookbooks) æ¥è¿›è¡Œèƒ½åŠ›å±•ç¤º, åŒ…æ‹¬éŸ³é¢‘ç†è§£ã€è¯­éŸ³å¯¹è¯ã€å±å¹•å½•åˆ¶äº¤äº’ã€è§†é¢‘ä¿¡æ¯æå–ã€å¤šæ¨¡æ€å¯¹è¯ç­‰ç­‰ï¼Œæ•¬è¯·è®¿é—®ã€‚

### ğŸ¤—  Transformers ä½¿ç”¨æ–¹æ³•

åœ¨è¿™é‡Œæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç®€å•çš„ä»£ç ç‰‡æ®µæ¥å‘æ‚¨å±•ç¤ºå¦‚ä½•é€šè¿‡`transformers` and `qwen_omni_utils`æ¥ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹:

```python
import soundfile as sf

from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor
from qwen_omni_utils import process_mm_info

# default: Load the model on the available device(s)
model = Qwen2_5OmniForConditionalGeneration.from_pretrained("Qwen/Qwen2.5-Omni-7B", torch_dtype="auto", device_map="auto")

# æˆ‘ä»¬å»ºè®®å¯ç”¨ flash_attention_2 ä»¥è·å–æ›´å¿«çš„æ¨ç†é€Ÿåº¦ä»¥åŠæ›´ä½çš„æ˜¾å­˜å ç”¨.
# model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
#     "Qwen/Qwen2.5-Omni-7B",
#     torch_dtype="auto",
#     device_map="auto",
#     attn_implementation="flash_attention_2",
# )

processor = Qwen2_5OmniProcessor.from_pretrained("Qwen/Qwen2.5-Omni-7B")

conversation = [
    {
        "role": "system",
        "content": [
            {"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}
        ],
    },
    {
        "role": "user",
        "content": [
            {"type": "video", "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/draw.mp4"},
        ],
    },
]

# set use audio in video
USE_AUDIO_IN_VIDEO = True

# Preparation for inference
text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors="pt", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = inputs.to(model.device).to(model.dtype)

# Inference: Generation of the output text and audio
text_ids, audio = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO)

text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
print(text)
sf.write(
    "output.wav",
    audio.reshape(-1).detach().cpu().numpy(),
    samplerate=24000,
)
```

#### æœ€å°GPUå†…å­˜éœ€æ±‚

| æ¨¡å‹ | ç²¾åº¦ | 15(s) è§†é¢‘ | 30(s) è§†é¢‘ | 60(s) è§†é¢‘ |
|--------------|-----------| ------------- | ------------- | ------------------ |
| Qwen-Omni-3B | FP32      | 89.10 GB      | ä¸æ¨è | ä¸æ¨è      |
| Qwen-Omni-3B | BF16      | 18.38 GB      | 22.43 GB      | 28.22 GB           |
| Qwen-Omni-7B | FP32      | 93.56 GB      | ä¸æ¨è | ä¸æ¨è      |
| Qwen-Omni-7B | BF16      | 31.11 GB      | 41.85 GB      | 60.19 GB           |

æ³¨æ„: ä¸Šè¿°çš„è¡¨æ ¼æ‰€å±•ç¤ºçš„åªæ˜¯ä½¿ç”¨`transformers`è¿›è¡Œæ¨ç†çš„ç†è®ºæœ€å°å€¼ï¼Œå¹¶ä¸”`BF16`çš„ç»“æœæ˜¯åœ¨`attn_implementation="flash_attention_2"`çš„æƒ…å†µä¸‹æµ‹è¯•å¾—åˆ°çš„ï¼Œä½†æ˜¯åœ¨å®é™…ä¸­ï¼Œå†…å­˜ä½¿ç”¨é€šå¸¸æ¯”è¿™ä¸ªå€¼è¦é«˜è‡³å°‘1.2å€ã€‚ æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[è¿™é‡Œ](https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator)ã€‚å¹¶ä¸”æˆ‘ä»¬ç›®å‰å·²ç»åœ¨è§„åˆ’ç ”å‘èƒ½å¤Ÿåœ¨è¾ƒä½èµ„æºæ¶ˆè€—éœ€æ±‚ä¸‹è¿›è¡Œæ¨ç†çš„ç‰ˆæœ¬ï¼Œä»¥ä¾¿äºåœ¨å¤§å¤šæ•°å¹³å°ä¸Šèƒ½å¤Ÿè¿è¡ŒQwen2.5-Omniï¼Œæ•¬è¯·æœŸå¾…ï¼

<details>
<summary>è§†é¢‘URLè¶…é“¾æ¥ä½¿ç”¨æ–¹æ³•</summary>

è§†é¢‘URLè¶…é“¾æ¥çš„å…¼å®¹æ€§å–å†³äºç¬¬ä¸‰æ–¹åº“çš„ç‰ˆæœ¬ã€‚å…·ä½“çš„ç»†èŠ‚åœ¨ä¸‹è¡¨ä¸­æ‰€ç¤ºï¼Œå¦‚æœæ‚¨ä¸å¸Œæœ›ä½¿ç”¨é»˜è®¤å€¼ï¼Œå¯ä»¥é€šè¿‡è®¾ç½®`FORCE_QWENVL_VIDEO_READER=torchvision`æˆ–`FORCE_QWENVL_VIDEO_READER=decord`æ¥å®ç°ã€‚

| åç«¯ç±»å‹     | HTTP | HTTPS |
|-------------|------|-------|
| torchvision >= 0.19.0 | âœ…  | âœ…   |
| torchvision < 0.19.0  | âŒ  | âŒ   |
| decord      | âœ…  | âŒ   |
</details>

<details>
<summary>æ‰¹å¤„ç†</summary>

å½“`return_audio=False`è®¾ç½®æ—¶ï¼Œæ¨¡å‹æ”¯æŒæ··åˆè¾“å…¥çš„æ‰¹å¤„ç†ï¼Œå…¶ä¸­å¯ä»¥åŒ…å«å„ç§ç±»å‹çš„æ ·æœ¬ï¼Œå¦‚æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªä»£ç ç‰‡æ®µçš„ç¤ºä¾‹ã€‚

```python
# Sample messages for batch inference

# Conversation with video only
conversation1 = [
    {
        "role": "system",
        "content": [
            {"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}
        ],
    },
    {
        "role": "user",
        "content": [
            {"type": "video", "video": "/path/to/video.mp4"},
        ]
    }
]

# Conversation with audio only
conversation2 = [
    {
        "role": "system",
        "content": [
            {"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}
        ],
    },
    {
        "role": "user",
        "content": [
            {"type": "audio", "audio": "/path/to/audio.wav"},
        ]
    }
]

# Conversation with pure text
conversation3 = [
    {
        "role": "system",
        "content": [
            {"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}
        ],
    },
    {
        "role": "user",
        "content": "who are you?"
    }
]


# Conversation with mixed media
conversation4 = [
    {
        "role": "system",
        "content": [
            {"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}
        ],
    },
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "/path/to/image.jpg"},
            {"type": "video", "video": "/path/to/video.mp4"},
            {"type": "audio", "audio": "/path/to/audio.wav"},
            {"type": "text", "text": "What are the elements can you see and hear in these medias?"},
        ],
    }
]

# Combine messages for batch processing
conversations = [conversation1, conversation2, conversation3, conversation4]

# set use audio in video
USE_AUDIO_IN_VIDEO = True

# Preparation for batch inference
text = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)

inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors="pt", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = inputs.to(model.device).to(model.dtype)

# Batch Inference
text_ids = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO, return_audio=False)
text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
print(text)
```
</details>


### ğŸ¤– ModelScope ä½¿ç”¨æ–¹æ³•
æˆ‘ä»¬å¼ºçƒˆå»ºè®®ä¸­å›½ç”¨æˆ·ä½¿ç”¨ModelScopeæ¥è·å–æ¨¡å‹ï¼Œ`snapshot_download`å¯ä»¥è§£å†³ä¸‹è½½è¿‡ç¨‹ä¸­çš„å„ç§ç½‘ç»œé—®é¢˜ï¼Œè¯¦æƒ…è¯·å‚è€ƒ[ModelScope](https://modelscope.cn/organization/qwen)ã€‚

### GPTQ-Int4 å’Œ AWQ ä½¿ç”¨æ–¹æ³•

ä¸ºäº†æé«˜Qwen2.5-Omni-7Båœ¨æ˜¾å­˜æœ‰é™çš„å¹³å°ä¸Šè¿è¡Œçš„çš„å¯èƒ½æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨GPTQå’ŒAWQå¯¹Thinkeræƒé‡è¿›è¡Œ4-bité‡åŒ–ï¼Œä»è€Œæ˜¾è‘—å‡å°‘æ˜¾å­˜ä½¿ç”¨é‡ï¼Œå…¶ä»–å…³é”®ä¼˜åŒ–åŒ…æ‹¬ï¼š

* æ”¹å–„æ¨ç†æµç¨‹ï¼Œæ ¹æ®æ¯ä¸ªæ¨¡å—çš„éœ€æ±‚æ¥ä¼˜åŒ–æ¨¡å‹æƒé‡çš„åŠ è½½æ–¹å¼ï¼Œä»¥å‡å°‘æ˜¾å­˜ä½¿ç”¨é‡
* å°†code2wavæ¨¡å—è½¬æ¢ä¸ºæ”¯æŒæµå¼æ¨ç†ï¼Œä»è€Œé¿å…é¢„åˆ†é…è¿‡å¤§çš„æ˜¾å­˜ã€‚
* å°†ODEæ±‚è§£å™¨ä»å››é˜¶æ–¹æ³•æ”¹ä¸ºä¸€é˜¶æ–¹æ³•ï¼Œä»¥å‡å°‘è®¡ç®—å¼€é”€

è¿™äº›æå‡ä¸»è¦ç›®çš„æ˜¯ä¸ºäº†ç¡®ä¿Qwen2.5-Omniå¯ä»¥è¿è¡Œåœ¨ä¸€äº›ä½æ˜¾å­˜è®¾å¤‡ä¸Šè¿è¡Œï¼Œä¾‹å¦‚RTX3080ã€4080ã€5070ç­‰ã€‚ç›®å‰ï¼Œç›¸å…³çš„æ¨¡å‹å’Œä½¿ç”¨æ–¹æ³•å¯ä»¥ä»Hugging Face ([GPTQ-Int4](https://huggingface.co/Qwen/Qwen2.5-Omni-7B-GPTQ-Int4)|[AWQ](https://huggingface.co/Qwen/Qwen2.5-Omni-7B-AWQ)) å’Œ ModelScope ([GPTQ-Int4](https://modelscope.cn/models/Qwen/Qwen2.5-Omni-7B-GPTQ-Int4)|[AWQ](https://modelscope.cn/models/Qwen/Qwen2.5-Omni-7B-AWQ))ä¸Šæ¥è·å–ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬æä¾›ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼Œä»¥å±•ç¤ºå¦‚ä½•é€šè¿‡`gptqmodel`æ¥ä½¿ç”¨Qwen2.5-Omni-7B-GPTQ-Int4æ¨¡å‹ï¼š
```
pip install transformers==4.52.3
pip install accelerate
pip install gptqmodel==2.0.0
pip install numpy==2.0.0

git clone https://github.com/QwenLM/Qwen2.5-Omni.git

cd Qwen2.5-Omni/low-VRAM-mode/

CUDA_VISIBLE_DEVICES=0 python3 low_VRAM_demo_gptq.py
```

è¦é€šè¿‡`autoawq`æ¥ä½¿ç”¨Qwen2.5-Omni-7B-AWQ withï¼Œè¯·æ‰§è¡Œ:
```
pip install transformers==4.52.3
pip install accelerate
pip install autoawq==0.2.9

git clone https://github.com/QwenLM/Qwen2.5-Omni.git

cd Qwen2.5-Omni/low-VRAM-mode/

CUDA_VISIBLE_DEVICES=0 python3 low_VRAM_demo_awq.py
```

ä¸‹é¢çš„ä¸¤å¼ è¡¨å±•ç¤ºäº†Qwen2.5-Omni-7B-GPTQ-Int4/Qwen2.5-Omni-7B-AWQ ä¸Qwen2.5-Omni-7Båœ¨éƒ¨åˆ†è¯„æµ‹é›†åˆä¸Šçš„æŒ‡æ ‡å¯¹æ¯”ä»¥åŠGPUæ˜¾å­˜å ç”¨ï¼Œä»ä¸­å¯ä»¥å¾—å‡ºï¼Œä½¿ç”¨GPTQ-Int4/AWQè¿›è¡Œé‡åŒ–åçš„æ¨¡å‹å¯¹æ€§èƒ½çš„æŸå¤±è¾ƒå°ï¼Œä½†GPUæ˜¾å­˜éœ€æ±‚é™ä½è¶…è¿‡50%+ï¼Œè¿™ä½¿å¾—æ›´å¤šè®¾å¤‡èƒ½å¤Ÿè¿è¡Œå¹¶ä½“éªŒåˆ°é«˜æ€§èƒ½çš„Qwen2.5-Omni-7BåŸç‰ˆæ¨¡å‹ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒGPTQ-Int4/AWQæ¨¡å‹åœ¨æ¨ç†é€Ÿåº¦ä¸Šä¸åŸç‰ˆæ¨¡å‹ç›¸æ¯”ç¨æ…¢ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºé‡åŒ–æŠ€æœ¯ä»¥åŠCPU offloadæœºåˆ¶å¯¼è‡´çš„ã€‚

| Evaluation Set | Task | Metrics | Qwen2.5-Omni-7B | Qwen2.5-Omni-7B-GPTQ-Int4 | Qwen2.5-Omni-7B-AWQ |
|--------------|-----------| ------------- | ------------- | ------------------ |  ------------------ |
| LibriSpeech test-other   | ASR                   | WER â¬‡ï¸      | 3.4   | 3.71  | 3.91  |
| WenetSpeech test-net     | ASR                   | WER â¬‡ï¸      | 5.9   | 6.62  | 6.31  |
| Seed-TTS test-hard       | TTS (Speaker: Chelsie)| WER â¬‡ï¸      | 8.7   | 10.3  | 8.88  |
| MMLU-Pro                 | Text -> Text          | Accuracy â¬†ï¸ | 47.0  | 43.76 | 45.66 |
| OmniBench                | Speech -> Text        | Accuracy â¬†ï¸ | 56.13 | 53.59 | 54.64 |
| VideoMME                 | Multimodality -> Text | Accuracy â¬†ï¸ | 72.4  | 68.0  | 72.0  |

|Model | Precision | 15(s) Video | 30(s) Video | 60(s) Video |
|--------------|-----------| ------------- | ------------- | ------------------ |
| Qwen-Omni-7B | FP32      | 93.56 GB      | Not Recommend | Not Recommend      |
| Qwen-Omni-7B | BF16      | 31.11 GB      | 41.85 GB      | 60.19 GB           |
| Qwen-Omni-7B | GPTQ-Int4 | 11.64 GB      | 17.43 GB      | 29.51 GB           |
| Qwen-Omni-7B | AWQ       | 11.77 GB      | 17.84 GB      | 30.31 GB           |

### ä½¿ç”¨æç¤º

#### éŸ³é¢‘è¾“å‡ºçš„æç¤ºè¯
å¦‚æœç”¨æˆ·éœ€è¦éŸ³é¢‘è¾“å‡ºï¼Œé‚£ä¹ˆç³»ç»Ÿæç¤ºå¿…é¡»è®¾ç½®ä¸º"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."ï¼Œå¦åˆ™éŸ³é¢‘è¾“å‡ºå¯èƒ½ä¸ä¼šæŒ‰ç…§é¢„æœŸå·¥ä½œã€‚
```
{
    "role": "system",
    "content": [
        {"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}
    ]
}
```
#### ä½¿ç”¨è§†é¢‘ä¸­çš„éŸ³é¢‘
åœ¨å¤šæ¨¡æ€äº¤äº’è¿‡ç¨‹ä¸­ï¼Œç”¨æˆ·æä¾›çš„è§†é¢‘é€šå¸¸ä¼´éšç€éŸ³é¢‘ï¼ˆå¦‚å¯¹è§†é¢‘å†…å®¹çš„æé—®ï¼Œæˆ–è€…è§†é¢‘ä¸­æŸäº›äº‹ä»¶çš„å£°éŸ³ï¼‰ï¼Œè¿™äº›ä¿¡æ¯å¯¹æ¨¡å‹çš„äº¤äº’ä½“éªŒå¾ˆå…³é”®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æä¾›äº†ä»¥ä¸‹é€‰é¡¹è®©ç”¨æˆ·å†³å®šæ˜¯å¦ä½¿ç”¨è§†é¢‘ä¸­çš„éŸ³é¢‘ã€‚
```python
# ç¬¬ä¸€å¤„ï¼Œåœ¨æ•°æ®é¢„å¤„ç†ä¸­
audios, images, videos = process_mm_info(conversations, use_audio_in_video=True)
```
```python
# ç¬¬äºŒå¤„ï¼Œåœ¨æ¨¡å‹å¤„ç†ä¸­
inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors="pt", 
                   padding=True, use_audio_in_video=True)
```
```python
# ç¬¬ä¸‰å¤„ï¼Œåœ¨æ¨¡å‹æ¨ç†ä¸­
text_ids, audio = model.generate(**inputs, use_audio_in_video=True)
```
å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨å¤šè½®å¯¹è¯è¿‡ç¨‹ä¸­ï¼Œ`use_audio_in_video`å‚æ•°åœ¨è¿™å‡ ä¸ªåœ°æ–¹å¿…é¡»è®¾ç½®ä¸ºç›¸åŒçš„å€¼ï¼Œå¦åˆ™å¯èƒ½ä¼šå‡ºç°éé¢„æœŸçš„ç»“æœã€‚

#### æ˜¯å¦ä½¿ç”¨éŸ³é¢‘è¾“å‡º
æ¨¡å‹æ”¯æŒæ–‡æœ¬å’ŒéŸ³é¢‘è¾“å‡ºï¼Œå¦‚æœç”¨æˆ·ä¸éœ€è¦éŸ³é¢‘è¾“å‡ºï¼Œå¯ä»¥åœ¨æ¨¡å‹åŠ è½½å®Œæ¯•åè°ƒç”¨`model.disable_talker()`ï¼Œæ­¤é€‰é¡¹å°†èŠ‚çœçº¦`2GB`çš„GPUå†…å­˜ï¼Œä½†`generate`å‡½æ•°çš„`return_audio`é€‰é¡¹å°†åªèƒ½è®¾ç½®ä¸º`False`ã€‚

```python
model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-Omni-7B",
    torch_dtype="auto",
    device_map="auto"
)
model.disable_talker()
```

ä¸ºäº†è·å¾—çµæ´»çš„ä½“éªŒï¼Œæˆ‘ä»¬å»ºè®®åœ¨è°ƒç”¨`generate`å‡½æ•°æ—¶æ ¹æ®éœ€è¦å†³å®šæ˜¯å¦è¿”å›éŸ³é¢‘ã€‚å½“`return_audio`è®¾ç½®ä¸º`False`æ—¶ï¼Œæ¨¡å‹å°†ä»…è¿”å›æ–‡æœ¬è¾“å‡ºä»¥æ›´å¿«åœ°è·å–æ–‡æœ¬å“åº”ã€‚

```python
model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-Omni-7B",
    torch_dtype="auto",
    device_map="auto"
)
...
text_ids = model.generate(**inputs, return_audio=False)
```

#### ä¿®æ”¹è¾“å‡ºè¯­éŸ³çš„éŸ³è‰²ç±»å‹
Qwen2.5-Omni æ”¯æŒä¿®æ”¹è¾“å‡ºè¯­éŸ³çš„éŸ³è‰²ç±»å‹ï¼Œ`"Qwen/Qwen2.5-Omni-7B"`ç›®å‰æ”¯æŒä¸¤ç§å¦‚ä¸‹ä¸¤ç§éŸ³è‰²ç±»å‹ï¼š


| éŸ³è‰²ç±»å‹ | æ€§åˆ« | æè¿° |
|------------|--------|-------------|
| Chelsie    | å¥³ | ç”œç¾ï¼Œæ¸©å©‰ï¼Œæ˜äº®ï¼Œè½»æŸ”|
| Ethan      | ç”·   | é˜³å…‰ï¼Œæ´»åŠ›ï¼Œè½»å¿«ï¼Œäº²å’Œ|

ç”¨æˆ·å¯ä»¥ä½¿ç”¨`generate`å‡½æ•°çš„`speaker`å‚æ•°æ¥æŒ‡å®šéŸ³è‰²ç±»å‹ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®š`speaker`ï¼Œåˆ™é»˜è®¤éŸ³è‰²ç±»å‹ä¸º`Chelsie`ã€‚

```python
text_ids, audio = model.generate(**inputs, speaker="Chelsie")
```

```python
text_ids, audio = model.generate(**inputs, speaker="Ethan")
```

#### ä½¿ç”¨Flash-Attention 2åŠ é€Ÿç”Ÿæˆ

é¦–å…ˆï¼Œè¯·ç¡®ä¿å·²å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„Flash Attention 2ï¼š

```bash
pip install -U flash-attn --no-build-isolation
```

å½“ç„¶ï¼Œæ‚¨è¿˜éœ€è¦ç¡®ä¿æ‚¨çš„ç¡¬ä»¶å…¼å®¹Flash Attention 2ï¼Œå¯ä»¥ä»[å®˜æ–¹æ–‡æ¡£](https://github.com/Dao-AILab/flash-attention)æ¥è·å–æ›´å¤šä¿¡æ¯ã€‚FlashAttention-2ä»…å¯åœ¨æ¨¡å‹åŠ è½½åˆ°`torch.float16`æˆ–`torch.bfloat16`æ—¶ä½¿ç”¨ã€‚

ä¸ºäº†åŠ è½½å¹¶è¿è¡Œä½¿ç”¨FlashAttention-2çš„æ¨¡å‹ï¼Œåœ¨åŠ è½½æ¨¡å‹æ—¶æ·»åŠ `attn_implementation="flash_attention_2"`ï¼š

```python
from transformers import Qwen2_5OmniForConditionalGeneration

model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-Omni-7B",
    device_map="auto",
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
)
```


### æ›´å¤šä½¿ç”¨ç”¨ä¾‹çš„ Cookbooks

| Cookbook | æè¿° | æ‰“å¼€ |
| -------- | ----------- | ---- |
| [é€šç”¨è¯­éŸ³ç†è§£](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/universal_audio_understanding.ipynb) | è¯­éŸ³è¯†åˆ«ï¼Œè¯­éŸ³åˆ°æ–‡æœ¬ç¿»è¯‘ï¼ŒéŸ³é¢‘åˆ†æã€‚ | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/universal_audio_understanding.ipynb) |
| [è¯­éŸ³å¯¹è¯](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/voice_chatting.ipynb) | å’Œ Qwen2.5-Omin é€šè¿‡è¯­éŸ³è¾“å…¥å’Œè¾“å‡ºå¯¹è¯ã€‚ | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/voice_chatting.ipynb) |
| [å±å¹•å½•åˆ¶äº¤äº’](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/screen_recording_interaction.ipynb) | ä»æ­£åœ¨å®æ—¶å½•åˆ¶çš„å±å¹•ä¸­é€šè¿‡æé—®çš„æ–¹å¼è·å–æƒ³äº†è§£çš„ä¿¡æ¯å’Œå†…å®¹ã€‚ | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/screen_recording_interaction.ipynb) |
| [è§†é¢‘ä¿¡æ¯æå–](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/video_information_extracting.ipynb) | ä»è§†é¢‘æµä¸­è·å–ä¿¡æ¯ã€‚ | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/video_information_extracting.ipynb) |
| [å…³äºéŸ³ä¹çš„å…¨æ¨¡æ€å¯¹è¯](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/omni_chatting_for_music.ipynb) | å’Œ Qwen2.5-Omni é€šè¿‡éŸ³è§†é¢‘æµçš„äº¤äº’æ–¹å¼èŠèŠå…³äºéŸ³ä¹çš„è¯é¢˜ã€‚ | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/omni_chatting_for_music.ipynb) |
| [å…³äºæ•°å­¦çš„å…¨æ¨¡æ€å¯¹è¯](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/omni_chatting_for_math.ipynb) | å’Œ Qwen2.5-Omni é€šè¿‡éŸ³è§†é¢‘æµçš„äº¤äº’æ–¹å¼èŠèŠå…³äºæ•°å­¦çš„è¯é¢˜ã€‚ | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/omni_chatting_for_math.ipynb) |
| [å¤šè½®å…¨æ¨¡æ€å¯¹è¯](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/multi_round_omni_chatting.ipynb) |  ä¸ Qwen2.5-Omni è¿›è¡Œäº†å¤šè½®å…¨æ¨¡æ€çš„éŸ³è§†é¢‘å¯¹è¯ï¼Œæä¾›æœ€å…¨é¢çš„èƒ½åŠ›æ¼”ç¤ºã€‚ | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/multi_round_omni_chatting.ipynb) |

### API æ¨ç†

ä¸ºäº†è¿‘ä¸€æ­¥æ¢ç´¢ Qwen2.5-Omniï¼Œæˆ‘ä»¬é¼“åŠ±æ‚¨ä½¿ç”¨æˆ‘ä»¬çš„æœ€æ–° API æœåŠ¡æ¥è·å–æ›´å¿«æ›´é«˜æ•ˆçš„ä½“éªŒã€‚

#### å®‰è£…
```bash
pip install openai
```

#### ç¤ºä¾‹
æ‚¨å¯ä»¥æŒ‰ç…§å¦‚ä¸‹çš„ç¤ºä¾‹ä½¿ç”¨ OpenAI API æœåŠ¡ä¸ Qwen2.5-Omni è¿›è¡Œäº¤äº’ï¼Œå¯¹äºæ›´å¤šçš„ä½¿ç”¨æ–¹æ³•ï¼Œè¯·å‚è€ƒé˜¿é‡Œäº‘çš„[æ•™ç¨‹](https://help.aliyun.com/zh/model-studio/user-guide/qwen-omni)ã€‚
```python
import base64
import numpy as np
import soundfile as sf

from openai import OpenAI

client = OpenAI(
    api_key="your_api_key",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

messages = [
    {
        "role": "system",
        "content": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.",
    },
    {
        "role": "user",
        "content": [
            {"type": "video_url", "video_url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/draw.mp4"},
        ],
    },
]

# Qwen-Omni only supports stream mode
completion = client.chat.completions.create(
    model="qwen-omni-turbo",
    messages=messages,
    modalities=["text", "audio"],
    audio={
        "voice": "Cherry", # Cherry, Ethan, Serena, Chelsie is available
        "format": "wav"
    },
    stream=True,
    stream_options={"include_usage": True}
)

text = []
audio_string = ""
for chunk in completion:
    if chunk.choices:
        if hasattr(chunk.choices[0].delta, "audio"):
            try:
                audio_string += chunk.choices[0].delta.audio["data"]
            except Exception as e:
                text.append(chunk.choices[0].delta.audio["transcript"])
    else:
        print(chunk.usage)

print("".join(text))
wav_bytes = base64.b64decode(audio_string)
wav_array = np.frombuffer(wav_bytes, dtype=np.int16)
sf.write("output.wav", wav_array, samplerate=24000)
```

### è‡ªå®šä¹‰æ¨¡å‹è®¾å®š

ç”±äºQwen2.5-Omniåœ¨ä½¿ç”¨[éŸ³é¢‘è¾“å‡º](#éŸ³é¢‘è¾“å‡ºçš„æç¤ºè¯)æ—¶ï¼ˆåŒ…æ‹¬æœ¬åœ°éƒ¨ç½²å’ŒAPIæ¨ç†ï¼‰å¹¶ä¸æ”¯æŒpromptçš„è®¾å®šï¼Œå› æ­¤æˆ‘ä»¬å»ºè®®å¦‚æœæ‚¨éœ€è¦å¯¹æ¨¡å‹çš„è¾“å‡ºè¿›è¡Œä¸€å®šçš„æ§åˆ¶æˆ–è€…å¯¹æ¨¡å‹çš„äººè®¾ç­‰è¿›è¡Œä¿®æ”¹ï¼Œå¯ä»¥å°è¯•åœ¨å¯¹è¯æ¨¡æ¿ä¸­åŠ å…¥å¦‚ä¸‹ç±»ä¼¼çš„è®¾å®šï¼š

```python
conversation = [
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "You are a shopping guide, now responsible for introducing various products."},
        ],
    },
    {
        "role": "assistant",
        "content": [
            {"type": "text", "text": "Sure, I got it."},
        ],
    },
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "Who are you?"},
        ],
    },
]
```

## å’Œ Qwen2.5-Omni å¯¹è¯

### åœ¨çº¿æ¼”ç¤º

æ— éœ€éƒ¨ç½²ï¼Œæ‚¨å¯ä»¥ç›´æ¥è®¿é—®æˆ‘ä»¬çš„ [Hugginface Spaces](https://huggingface.co/spaces/Qwen/Qwen2.5-Omni-7B-Demo) å’Œ [Modelscope åˆ›ç©ºé—´](https://modelscope.cn/studios/Qwen/Qwen2.5-Omni-Demo) æ¥ç›´æ¥ä½“éªŒåœ¨çº¿ç½‘é¡µæ¼”ç¤ºã€‚

### å¯åŠ¨æœ¬åœ°ç½‘é¡µæ¼”ç¤º

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†å¦‚ä½•æ„å»ºä¸€ä¸ªåŸºäºç½‘é¡µçš„ UIï¼ˆç”¨æˆ·ç•Œé¢ï¼‰æ¼”ç¤ºçš„æŒ‡å—ï¼Œæ­¤ UI æ¼”ç¤ºå…è®¸ç”¨æˆ·é€šè¿‡æµè§ˆå™¨ä¸é¢„å®šä¹‰çš„æ¨¡å‹æˆ–åº”ç”¨ç¨‹åºè¿›è¡Œäº¤äº’ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å¼€å§‹ä½¿ç”¨æˆ–æ‚¨å¯ä»¥ç›´æ¥ä»æˆ‘ä»¬çš„[å®˜æ–¹ Docker é•œåƒ](#-docker)ä¸­å¯åŠ¨ç½‘é¡µæ¼”ç¤ºã€‚

#### å®‰è£…

åœ¨æ‚¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€éœ€çš„ä¾èµ–é¡¹ï¼Œæ‚¨å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…å®ƒä»¬ï¼š

```bash
pip install -r requirements_web_demo.txt
```

#### åŸºäº FlashAttention-2 å¯åŠ¨æ¼”ç¤º

ä¸€æ—¦æ‚¨å·²å®‰è£…æ‰€éœ€çš„ä¾èµ–é¡¹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨ç½‘é¡µæ¼”ç¤ºã€‚æ­¤å‘½ä»¤å°†å¯åŠ¨ä¸€ä¸ª Web æœåŠ¡ï¼Œå¹¶æä¾›æ‚¨ç”¨äºåœ¨ Web æµè§ˆå™¨ä¸­è®¿é—® UI çš„é“¾æ¥ã€‚

**å»ºè®®**: ä¸ºäº†è·å¾—æ›´å¥½çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œå°¤å…¶æ˜¯å¤„ç†å¤§é‡å›¾åƒå’Œè§†é¢‘çš„åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®ä½¿ç”¨ [FlashAttention-2](https://github.com/Dao-AILab/flash-attention)ã€‚FlashAttention-2 æä¾›äº†æ˜¾å­˜ä½¿ç”¨å’Œé€Ÿåº¦çš„æ˜¾è‘—æ”¹è¿›ï¼Œå› æ­¤å¯¹äºå¤„ç†å¤§å‹æ¨¡å‹å’Œæ•°æ®å¤„ç†çš„åœºæ™¯ï¼Œå®ƒéå¸¸åˆé€‚ã€‚

ä¸ºäº†å¯ç”¨ FlashAttention-2ï¼Œä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å¯åŠ¨æ¼”ç¤ºï¼š

```bash
# default for Qwen2.5-Omni-7B
python web_demo.py --flash-attn2
```
```bash
# for Qwen2.5-Omni-3B
python web_demo.py --flash-attn2 -c Qwen/Qwen2.5-Omni-3B
```

è¿™å°†ä¼šåŠ è½½æ¨¡å‹å¹¶å¯ç”¨ FlashAttention-2ã€‚

**é»˜è®¤ä½¿ç”¨æ–¹æ³•**: å¦‚æœæ‚¨æ›´å¸Œæœ›åœ¨ä¸ä½¿ç”¨ FlashAttention-2 çš„æƒ…å†µä¸‹è¿è¡Œæ¼”ç¤ºï¼Œä¹Ÿå³ä¸æŒ‡å®š`--flash-attn2`å‚æ•°ï¼Œæ­¤æ—¶æ¼”ç¤ºå°†ä¼šä½¿ç”¨é»˜è®¤çš„æ³¨æ„åŠ›æœºåˆ¶å®ç°æ–¹æ³•æ¥åŠ å™ªå’Œè¿è¡Œæ¨¡å‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š

```bash
# default for Qwen2.5-Omni-7B
python web_demo.py
```
```bash
# for Qwen2.5-Omni-3B
python web_demo.py -c Qwen/Qwen2.5-Omni-3B
```

åœ¨è¿è¡Œè¿™ä¸ªå‘½ä»¤ä¹‹å, æ‚¨å°†ä¼šåœ¨ç»ˆç«¯ä¸­çœ‹åˆ°ç±»ä¼¼çš„è¾“å‡ºï¼š

```
Running on local: http://127.0.0.1:7860/
```

å¤åˆ¶è¯¥é“¾æ¥ï¼Œå¹¶å°†å…¶ç²˜è´´åˆ°æµè§ˆå™¨ä¸­ï¼Œå³å¯è®¿é—®ç½‘é¡µæ¼”ç¤ºï¼Œåœ¨ç½‘é¡µä¸­æ‚¨å¯ä»¥è¾“å…¥æ–‡æœ¬ã€ä¸Šä¼ éŸ³é¢‘ã€å›¾åƒå’Œè§†é¢‘ï¼Œä»¥åŠåˆ‡æ¢è¾“å‡ºéŸ³è‰²ç±»å‹ç­‰åŠŸèƒ½ã€‚


### å®æ—¶äº¤äº’

å®æ—¶äº¤äº’ä½“éªŒç›®å‰å·²ç»ä¸Šçº¿ï¼Œè¯·æ‚¨è®¿é—®[Qwen Chat](https://chat.qwen.ai/)ï¼Œå¹¶åœ¨èŠå¤©æ¡†ä¸­é€‰æ‹©è¯­éŸ³/è§†é¢‘é€šè¯ï¼Œå³å¯ä½“éªŒä¸Qwen2.5-Omniçš„å®æ—¶äº¤äº’ã€‚


## ä½¿ç”¨vLLMéƒ¨ç½²

æˆ‘ä»¬å»ºè®®ä½¿ç”¨vLLMè¿›è¡ŒQwen2.5-Omniçš„å¿«é€Ÿéƒ¨ç½²å’Œæ¨ç†ï¼Œæ‚¨éœ€è¦ä»æˆ‘ä»¬æä¾›çš„[æºç ](https://github.com/fyabc/vllm/tree/qwen2_omni_public)æ„å»ºvLLMä»¥è·å–å¯¹Qwen2.5-Omniæ”¯æŒï¼Œæˆ–è€…ä½¿ç”¨æˆ‘ä»¬çš„[å®˜æ–¹ docker é•œåƒ](#-docker)ã€‚æ‚¨ä¹Ÿå¯ä»¥æŸ¥çœ‹[vLLMå®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/en/latest/serving/multimodal_inputs.html)ä»¥è·å–åœ¨çº¿æœåŠ¡å’Œç¦»çº¿æ¨ç†çš„æ›´å¤šä¿¡æ¯ã€‚

### å®‰è£…
```bash
git clone -b qwen2_omni_public https://github.com/fyabc/vllm.git
cd vllm
git checkout de8f43fbe9428b14d31ac5ec45d065cd3e5c3ee0
pip install setuptools_scm torchdiffeq resampy x_transformers qwen-omni-utils accelerate
pip install -r requirements/cuda.txt
pip install --upgrade setuptools wheel
pip install .
pip install transformers==4.52.3
```

### æœ¬åœ°ç¦»çº¿æ¨ç†

æ‚¨å¯ä»¥ä½¿ç”¨vLLMè¿›è¡ŒQwen2.5-Omniçš„æœ¬åœ°æ¨ç†ï¼Œæˆ‘ä»¬åœ¨vLLM[æºç ](https://github.com/fyabc/vllm/tree/qwen2_omni_public)ä¸­æä¾›äº†[ç¤ºä¾‹](https://github.com/fyabc/vllm/blob/qwen2_omni_public/examples/offline_inference/qwen2_5_omni/end2end.py)ï¼Œè¯¥ç¤ºä¾‹å¯ä»¥ç«¯åˆ°ç«¯çš„ç”ŸæˆéŸ³é¢‘ã€‚

```bash
# git clone -b qwen2_omni_public https://github.com/fyabc/vllm.git
# cd vllm
# git checkout de8f43fbe9428b14d31ac5ec45d065cd3e5c3ee0
# cd examples/offline_inference/qwen2_5_omni/

# only text output for single GPU
python end2end.py --model Qwen/Qwen2.5-Omni-7B --prompt audio-in-video-v2 --enforce-eager --thinker-only

# only text output for multi GPUs (example in 4 GPUs)
python end2end.py --model Qwen/Qwen2.5-Omni-7B --prompt audio-in-video-v2 --enforce-eager --thinker-only --thinker-devices [0,1,2,3] --thinker-gpu-memory-utilization 0.9 

# audio output for single GPU
python end2end.py --model Qwen/Qwen2.5-Omni-7B --prompt audio-in-video-v2 --enforce-eager --do-wave --voice-type Chelsie --warmup-voice-type Chelsie --output-dir output_wav

# audio output for multi GPUs (example in 4 GPUs)
python end2end.py --model Qwen/Qwen2.5-Omni-7B --prompt audio-in-video-v2 --enforce-eager --do-wave --voice-type Chelsie --warmup-voice-type Chelsie --thinker-devices [0,1] --talker-devices [2] --code2wav-devices [3] --thinker-gpu-memory-utilization 0.9 --talker-gpu-memory-utilization 0.9 --output-dir output_wav
```

### ä½¿ç”¨vLLM serve

æ‚¨è¿˜å¯ä»¥é€šè¿‡`pip install vllm>=0.8.5.post1`æ¥ä½¿ç”¨vLLM serveï¼Œç›®å‰vLLM serveä»…æ”¯æŒæ–‡æœ¬è¾“å‡ºã€‚æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤é€šè¿‡vLLMæ¥å¯åŠ¨APIæœåŠ¡ï¼š
```bash
# for single GPU
vllm serve /path/to/Qwen2.5-Omni-7B/ --port 8000 --host 127.0.0.1 --dtype bfloat16
# for multi GPUs (example in 4 GPUs)
vllm serve /path/to/Qwen2.5-Omni-7B/ --port 8000 --host 127.0.0.1 --dtype bfloat16 -tp 4
```
ç„¶åæ‚¨å¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç ä½¿ç”¨è¿™ä¸ªAPI(ç¤ºä¾‹é‡Œæ˜¯ä½¿ç”¨curlå‘½ä»¤):
```bash
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
    "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": [
        {"type": "image_url", "image_url": {"url": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png"}},
        {"type": "audio_url", "audio_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/cough.wav"}},
        {"type": "text", "text": "What is the text in the illustrate ans what it the sound in the audio?"}
    ]}
    ]
    }'
```


## ä½¿ç”¨MNNéƒ¨ç½²

Qwen2.5-Omniç›®å‰å·²ç»åœ¨MNNæ¡†æ¶ä¸­æ”¯æŒï¼Œä»¥å®ç°åœ¨è¾¹ç¼˜è®¡ç®—è®¾å¤‡ä¸Šçš„éƒ¨ç½²ï¼Œæ‚¨å¯ä»¥é€šè¿‡Hugging Face ([7B](https://huggingface.co/taobao-mnn/Qwen2.5-Omni-7B-MNN)|[3B](https://huggingface.co/taobao-mnn/Qwen2.5-Omni-3B-MNN)) æˆ– ModelScope ([7B](https://modelscope.cn/models/MNN/Qwen2.5-Omni-7B-MNN)|[3B](https://modelscope.cn/models/MNN/Qwen2.5-Omni-3B-MNN))ä¸‹è½½Qwen2.5-Omniçš„MNNæ¨¡å‹ï¼Œå¹¶æŸ¥çœ‹ä½¿ç”¨è¯´æ˜ã€‚æ‚¨è¿˜å¯ä»¥è®¿é—®[MNN](https://github.com/alibaba/MNN)ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ä¸‹è¡¨å±•ç¤ºäº†Qwen2.5-Omniåœ¨MNNæ¡†æ¶ä¸­ï¼Œåœ¨å‡ ç§ç§»åŠ¨SoCå¹³å°çš„å†…å­˜æ¶ˆè€—å’Œæ¨ç†é€Ÿåº¦åŸºå‡†æµ‹è¯•ç»“æœã€‚


| å¹³å° | Snapdragon 8 Gen 1 | Snapdragon 8 Elite | Snapdragon 8 Gen 1 | Snapdragon 8 Elite  |
|--------------|-----------| ------------- | ------------- | ------------------ |
| æ¨¡å‹å¤§å°   | 7B | 7B | 3B | 3B |
| å†…å­˜å³°å€¼  | 5.8G | 5.8G | 3.6G | 3.6G |
| Thinker Prefill Speed | 25.58 tok/s | 46.32 tok/s | 54.31 tok/s | 55.16 tok/s | 
| Thinker Decode Speed  |  8.35 tok/s | 11.52 tok/s | 15.84 tok/s | 23.31 tok/s | 
| Talker Prefill Speed  | 17.21 tok/s | 97.77 tok/s | 34.58 tok/s | 217.82 tok/s| 
| Talker Decode Speed   | 18.75 tok/s | 38.65 tok/s | 51.90 tok/s | 62.34 tok/s | 
| Code2wav Speed         |20.83 tok/s | 27.36 tok/s | 28.45 tok/s | 27.36 tok/s | 


## ğŸ³ Docker

ä¸ºäº†ç®€åŒ–éƒ¨ç½²è¿‡ç¨‹ï¼Œæˆ‘ä»¬æä¾›äº†é¢„æ„å»ºç¯å¢ƒï¼š[qwenllm/qwen-omni](https://hub.docker.com/r/qwenllm/qwen-omni)ï¼Œæ‚¨åªéœ€è¦å®‰è£…é©±åŠ¨å¹¶ä¸‹è½½æ¨¡å‹æ–‡ä»¶å³å¯å¯åŠ¨æ¼”ç¤ºã€‚

```bash
docker run --gpus all --ipc=host --network=host --rm --name qwen2.5-omni -it qwenllm/qwen-omni:2.5-cu121 bash
```

å¹¶ä¸”æ‚¨ä¹Ÿå¯ä»¥ç›´æ¥é€šè¿‡å¦‚ä¸‹å‘½ä»¤å¯åŠ¨ç½‘é¡µæ¼”ç¤º:
```bash
bash docker/docker_web_demo.sh --checkpoint /path/to/Qwen2.5-Omni-7B
```
å¦‚éœ€å¯ç”¨FlashAttention-2ï¼Œè¯·ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤:
```bash
bash docker/docker_web_demo.sh --checkpoint /path/to/Qwen2.5-Omni-7B --flash-attn2
```


## å¼•ç”¨

å¦‚æœæ‚¨åœ¨æ‚¨çš„ç ”ç©¶ä¸­æ„Ÿåˆ° Qwen2.5-Omni ä¸ºæ‚¨æä¾›äº†å¸®åŠ©ï¼ŒæœŸå¾…æ‚¨èƒ½ç»™ä¸€ä¸ª Star :star: å’Œå¼•ç”¨ :pencil: :)



```BibTeX

@article{Qwen2.5-Omni,
  title={Qwen2.5-Omni Technical Report},
  author={Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, Junyang Lin},
  journal={arXiv preprint arXiv:2503.20215},
  year={2025}
}
```

<br>
