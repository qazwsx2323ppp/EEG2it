import torch
import clip
import numpy as np
import os
from PIL import Image
from tqdm import tqdm

# ================= é…ç½®åŒºåŸŸ =================
# 1. ä½ çš„ EEG æ•°æ®é›†è·¯å¾„
EEG_PATH = "data/EEG_data/eeg_55_95_std.pth" 

# 2. ImageNet å›¾ç‰‡çš„æ ¹ç›®å½•
IMAGE_ROOT = "data/image_data"

# 3. è¾“å‡ºè·¯å¾„ (å»ºè®®æ”¹ä¸ªåä»¥ç¤ºåŒºåˆ«)
OUTPUT_IMG_PATH = "data/image_vectors_aligned.npy"
OUTPUT_TXT_PATH = "data/text_vectors_aligned.npy"

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CLIP_MODEL = "ViT-B/32"
# ===========================================

def main():
    print(f"æ­£åœ¨åŠ è½½ EEG æ•°æ®é›†å…ƒæ•°æ®: {EEG_PATH} ...")
    try:
        # åŠ è½½ .pth æ–‡ä»¶
        data = torch.load(EEG_PATH, map_location='cpu')
        
        # ã€å…³é”®æ­¥éª¤ã€‘ç›´æ¥è·å–å®˜æ–¹å®šä¹‰çš„å›¾ç‰‡é¡ºåº
        # å‚è€ƒ thought2text/datautils.py çš„é€»è¾‘
        if 'images' in data:
            target_images = data['images']
        else:
            # å…¼å®¹æŸäº›æ•°æ®é›†ç»“æ„å·®å¼‚ï¼Œæœ‰çš„å¯èƒ½åœ¨ 'dataset' å¤–éƒ¨
            print("è­¦å‘Šï¼šæœªåœ¨æ ¹ç›®å½•æ‰¾åˆ° 'images'ï¼Œå°è¯•åœ¨ dataset å†…éƒ¨æŸ¥æ‰¾ï¼ˆå¦‚æœé€‚ç”¨ï¼‰...")
            target_images = data.get('images', [])
            
        if not target_images:
            raise KeyError("æ— æ³•æ‰¾åˆ° 'images' åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ .pth æ–‡ä»¶ç»“æ„")

        print(f"âœ… æˆåŠŸè·å–å›¾ç‰‡åˆ—è¡¨ï¼Œå…± {len(target_images)} å¼ ã€‚")
        print(f"   Index 0 å¯¹åº”: {target_images[0]}")
        print(f"   Index 100 å¯¹åº”: {target_images[100]}")
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        return

    print(f"æ­£åœ¨åŠ è½½ CLIP æ¨¡å‹: {CLIP_MODEL} ...")
    # æ³¨æ„ï¼šç¡®ä¿å®‰è£…äº†æ­£ç¡®çš„ clip (pip install git+https://github.com/openai/CLIP.git)
    model, preprocess = clip.load(CLIP_MODEL, device=DEVICE)
    
    img_vectors = []
    txt_vectors = []
    
    print("ğŸš€ å¼€å§‹ç”Ÿæˆä¸¥æ ¼å¯¹é½çš„å‘é‡...")
    
    # éå†åˆ—è¡¨ï¼Œé¡ºåºç»å¯¹ä¸èƒ½ä¹±ï¼
    for img_name in tqdm(target_images):
        # 1. æ‹¼å‡‘å›¾ç‰‡è·¯å¾„
        # img_name é€šå¸¸æ˜¯ 'n02106662_123.JPEG'
        class_folder = img_name.split('_')[0] 
        
        # ä¼˜å…ˆå°è¯•ï¼šIMAGE_ROOT/class_folder/img_name
        full_path = os.path.join(IMAGE_ROOT, class_folder, img_name)
        
        # å®¹é”™é€»è¾‘ï¼šæœ‰çš„æ–‡ä»¶åå¯èƒ½æ²¡åç¼€ï¼Œæˆ–è€…è·¯å¾„ç»“æ„ä¸åŒ
        if not os.path.exists(full_path):
             # å°è¯•åŠ  .JPEG
             if not img_name.lower().endswith('.jpeg') and not img_name.lower().endswith('.jpg'):
                 test_path = os.path.join(IMAGE_ROOT, class_folder, img_name + '.JPEG')
                 if os.path.exists(test_path):
                     full_path = test_path
        
        # ---------------- å›¾åƒç¼–ç  ----------------
        try:
            image = Image.open(full_path).convert("RGB")
            image_input = preprocess(image).unsqueeze(0).to(DEVICE)
            
            with torch.no_grad():
                img_feat = model.encode_image(image_input)
                # å½’ä¸€åŒ– (CLIP æ ‡å‡†)
                img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)
                img_vectors.append(img_feat.cpu().numpy())
                
        except Exception as e:
            print(f"\n[è­¦å‘Š] å›¾ç‰‡è¯»å–å¤±è´¥: {img_name}")
            print(f"       è·¯å¾„: {full_path}")
            # å¡«å……é›¶å‘é‡ï¼Œç»ä¸èƒ½è·³è¿‡ï¼Œå¦åˆ™åç»­ç´¢å¼•ä¼šå…¨éƒ¨é”™ä½ï¼
            img_vectors.append(np.zeros((1, 512), dtype=np.float32))

        # ---------------- æ–‡æœ¬ç¼–ç  ----------------
        # ç®€å• prompt: "a photo of [CLASS_ID]"
        text_prompt = f"a photo of {class_folder}" 
        text_input = clip.tokenize([text_prompt]).to(DEVICE)
        
        with torch.no_grad():
            txt_feat = model.encode_text(text_input)
            txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)
            txt_vectors.append(txt_feat.cpu().numpy())

    # åˆå¹¶ä¿å­˜
    final_img_vecs = np.concatenate(img_vectors, axis=0)
    final_txt_vecs = np.concatenate(txt_vectors, axis=0)
    
    print(f"\nğŸ’¾ ä¿å­˜å‘é‡åˆ°ç¡¬ç›˜...")
    np.save(OUTPUT_IMG_PATH, final_img_vecs)
    np.save(OUTPUT_TXT_PATH, final_txt_vecs)
    print(f"âœ… å®Œæˆï¼ç”Ÿæˆäº† {len(final_img_vecs)} ä¸ªå¯¹é½å‘é‡ã€‚")

if __name__ == "__main__":
    main()